"""
å‹•çš„ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼APIã‹ã‚‰å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
"""

import os
from typing import List, Dict, Any
from datetime import datetime, timedelta
from api.logger import setup_logger

logger = setup_logger(__name__)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
_MODEL_CACHE: Dict[str, List[Dict[str, Any]]] = {}
_CACHE_EXPIRY: Dict[str, datetime] = {}
# ç’°å¢ƒå¤‰æ•°å¯¾å¿œ: MODEL_CACHE_TTLãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3600ç§’ï¼ˆ1æ™‚é–“ï¼‰
CACHE_TTL = timedelta(seconds=int(os.getenv("MODEL_CACHE_TTL", "3600")))


def get_gemini_models() -> List[Dict[str, Any]]:
    """
    Gemini APIã‹ã‚‰å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å‹•çš„ã«å–å¾—

    ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å¯¾å¿œ (2024):
    - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆ1æ™‚é–“TTLï¼‰
    - ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
    - ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼ˆèµ·å‹•æ™‚1å›ã®ã¿ï¼‰
    - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆsupported_generation_methodsï¼‰

    Returns:
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆã€‚å„ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®æ§‹é€ :
        {
            "id": "gemini/gemini-2.5-flash",
            "name": "gemini-2.5-flash",
            "provider": "Gemini API",
            "litellm_provider": "gemini",
            "supports_vision": True,
            "supports_json": True,
            "description": "...",
            "cost_per_1k_tokens": {"input": 0.0, "output": 0.0}
        }
    """
    cache_key = "gemini_models_v3"  # v3: æ•™è‚²ç”¨éæ¨å¥¨ãƒ¢ãƒ‡ãƒ«è¿½åŠ 

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if cache_key in _MODEL_CACHE:
        if datetime.now() < _CACHE_EXPIRY[cache_key]:
            cached_count = len(_MODEL_CACHE[cache_key])
            logger.info("ğŸ’¾ Using cached Gemini models (%d models)", cached_count)
            return _MODEL_CACHE[cache_key]

    # Gemini APIã‹ã‚‰å–å¾—ï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
    try:
        # æ–°ã—ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: google-genai (2024+ recommended)
        import google.genai as genai
        import time

        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("GEMINI_API_KEY not set")
            return []

        max_retries = 3
        models = []

        for attempt in range(max_retries):
            try:
                # google-genai æ–°SDKï¼ˆClient APIã‚’ä½¿ç”¨ï¼‰
                client = genai.Client(api_key=api_key)

                # client.models.list()ã§ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«ï¼‰
                #
                # ===== Gemini API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (2024_12+) =====
                # æ–°SDKãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ä»¥ä¸‹ã®å±æ€§ãŒåˆ©ç”¨å¯èƒ½:
                # - name: ãƒ¢ãƒ‡ãƒ«ID (ä¾‹: "models/gemini-2.5-flash")
                # - display_name: è¡¨ç¤ºå
                # - description: ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜
                # - supported_actions: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹æ©Ÿèƒ½ã®ãƒªã‚¹ãƒˆ (NEW)
                #   æ—§SDKã§ã¯ supported_generation_methods
                #   ä¾‹: ['generateContent', 'streamGenerateContent', ...]
                # - input_token_limit: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
                # - output_token_limit: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
                # - temperature, top_k, top_p: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                # - thinking: Thinkingæ©Ÿèƒ½ã®ã‚µãƒãƒ¼ãƒˆ (ä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ã®ã¿)
                # - endpoints: åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
                # - labels: ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ™ãƒ«ãƒ»ã‚¿ã‚°
                # ãã®ä»–: checkpoints, tuned_model_info, default_checkpoint_id, etc.
                # =====================================================
                for model in client.models.list():
                    # æ–°SDKã§ã¯supported_actionsã€æ—§SDKã§ã¯supported_generation_methods
                    methods = getattr(model, "supported_actions", None)
                    if methods is None:
                        methods = getattr(model, "supported_generation_methods", None)
                    if methods is None:
                        continue

                    model_name = model.name.split("/")[
                        -1
                    ]  # "models/gemini-pro" -> "gemini-pro"

                    # ãƒãƒ£ãƒƒãƒˆç”¨é€”ï¼ˆgenerateContentï¼‰ã‹ã©ã†ã‹ã§æ¨å¥¨åˆ¤å®š
                    is_recommended = "generateContent" in methods

                    # Visionå¯¾å¿œã®åˆ¤å®šï¼ˆåå‰ãƒ™ãƒ¼ã‚¹ï¼‰
                    # Gemini APIã¯Visionå¯¾å¿œã‹ã©ã†ã‹ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§å…¬é–‹ã—ã¦ã„ãªã„ãŸã‚ã€
                    # ãƒ¢ãƒ‡ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®šã™ã‚‹ã€‚éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«é™¤å¤–ã™ã‚‹æ–¹å¼ã€‚
                    # - gemmaç³»: è»½é‡OSSãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå°‚ç”¨ï¼‰
                    # - embedç³»: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå°‚ç”¨ï¼‰
                    # - aqa: Attributed Question Answeringï¼ˆãƒ†ã‚­ã‚¹ãƒˆå°‚ç”¨ï¼‰
                    NON_VISION_PATTERNS = ["gemma", "embed", "aqa"]
                    supports_vision = "generateContent" in methods and not any(
                        p in model_name.lower() for p in NON_VISION_PATTERNS
                    )

                    # ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ¤œå‡º
                    # å‘½åè¦å‰‡: Geminiç”»åƒãƒ¢ãƒ‡ãƒ«ã¯å…¨ã¦ "image" ã‚’å«ã‚€
                    # ä¾‹: gemini-2.5-flash-image, gemini-2.5-flash-image-preview
                    is_image_generation = "image" in model_name.lower()

                    models.append(
                        {
                            "id": f"gemini/{model_name}",
                            "name": model_name,
                            "provider": "Gemini API",
                            "litellm_provider": "gemini",
                            "supports_vision": supports_vision,
                            "supports_json": not is_image_generation,
                            "supports_image_generation": is_image_generation,
                            "description": getattr(model, "description", ""),
                            "recommended": is_recommended,
                            "supported_methods": list(methods),  # ãƒ‡ãƒãƒƒã‚°ç”¨
                            "cost_per_1k_tokens": {"input": 0.0, "output": 0.0},
                        }
                    )

                # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                break

            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = 2**attempt  # 1ç§’, 2ç§’, 4ç§’
                    logger.warning(
                        "âš ï¸ Retry %d/%d after %ds: %s: %s",
                        attempt + 1,
                        max_retries,
                        wait_time,
                        type(e).__name__,
                        e,
                    )
                    time.sleep(wait_time)
                else:
                    # æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—
                    logger.error(
                        "âŒ Failed after %d attempts: %s: %s",
                        max_retries,
                        type(e).__name__,
                        e,
                    )
                    raise

        if not models:
            logger.warning("No Gemini models found from API")
            return []

        logger.info("âœ… Fetched %d Gemini models from API", len(models))

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆ1æ™‚é–“TTLï¼‰
        _MODEL_CACHE[cache_key] = models
        _CACHE_EXPIRY[cache_key] = datetime.now() + CACHE_TTL

        return models

    except ImportError as e:
        logger.error("âŒ CRITICAL: google-genai package not installed: %s", e)
        logger.error("âš ï¸  Install with: pip install -U google-genai")
        logger.error("âš ï¸  Or run: pip install -r requirements.txt")
        return []
    except Exception as e:
        logger.error("Failed to fetch Gemini models: %s: %s", type(e).__name__, e)
        return []


def get_openai_models() -> List[Dict[str, Any]]:
    """
    OpenAI APIã‹ã‚‰å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å‹•çš„ã«å–å¾—

    ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å¯¾å¿œ:
    - APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰
    - APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆå„ªé›…ãªå¤±æ•—ï¼‰
    - ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆgpt-, o1-, chatgpt-ï¼‰
    - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆ1æ™‚é–“TTLï¼‰

    Returns:
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆAPIã‚­ãƒ¼ãªã—ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆï¼‰
    """
    cache_key = "openai_models_v1"

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if cache_key in _MODEL_CACHE:
        if datetime.now() < _CACHE_EXPIRY.get(cache_key, datetime.min):
            cached_count = len(_MODEL_CACHE[cache_key])
            logger.info("ğŸ’¾ Using cached OpenAI models (%d models)", cached_count)
            return _MODEL_CACHE[cache_key]

    # APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        logger.info("OPENAI_API_KEY not set, skipping OpenAI models")
        return []

    try:
        from openai import OpenAI

        client = OpenAI(api_key=api_key)

        models_list = []

        # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        all_models = client.models.list()

        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆç ”ç©¶çµæœã«åŸºã¥ãï¼‰
        chat_prefixes = ["gpt-", "o1-", "o3-", "o4-", "chatgpt-"]

        for model in all_models:
            # ãƒãƒ£ãƒƒãƒˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            if not any(model.id.startswith(prefix) for prefix in chat_prefixes):
                continue

            # Visionå¯¾å¿œåˆ¤å®š
            supports_vision = any(
                keyword in model.id
                for keyword in ["vision", "gpt-4o", "gpt-4-turbo", "gpt-4.5"]
            )

            # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«åˆ¤å®šï¼ˆæœ€æ–°ã®å®‰å®šç‰ˆï¼‰
            # æ˜ç¤ºçš„ã«éæ¨å¥¨ã®ã‚‚ã®ã ã‘ã‚’é™¤å¤–ã™ã‚‹æ–¹å¼ã«å¤‰æ›´
            # Fine-tunedãƒ¢ãƒ‡ãƒ«ã€å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€å®Ÿé¨“çš„ãƒ¢ãƒ‡ãƒ«ãªã©ã‚’é™¤å¤–
            not_recommended_patterns = [
                "ft:",  # Fine-tunedãƒ¢ãƒ‡ãƒ«
                "gpt-4-0613",  # å¤ã„GPT-4ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                "gpt-4-0314",
                "gpt-3.5-turbo-0301",
                "gpt-3.5-turbo-0613",
                "gpt-3.5-turbo-16k-0613",
                "-preview",  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆï¼ˆo1-previewãªã©ä¾‹å¤–ã‚ã‚Šï¼‰
                "gpt-5",  # æœªãƒªãƒªãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
                "gpt-image",  # å®Ÿé¨“çš„
                "chatgpt-image",  # å®Ÿé¨“çš„
            ]

            # ä¾‹å¤–çš„ã«æ¨å¥¨ã™ã‚‹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ‡ãƒ«
            recommended_previews = ["o1-preview", "o1-mini"]

            # åˆ¤å®šï¼šéæ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã—ãªã„ã‹ã€ã¾ãŸã¯ä¾‹å¤–ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹
            is_preview_exception = any(exc in model.id for exc in recommended_previews)
            has_not_recommended_pattern = any(
                pattern in model.id for pattern in not_recommended_patterns
            )

            recommended = (not has_not_recommended_pattern) or is_preview_exception

            # supported_methodsæ¨æ¸¬ï¼ˆOpenAI APIã¯æ©Ÿèƒ½ãƒªã‚¹ãƒˆã‚’è¿”ã•ãªã„ãŸã‚åå‰ã‹ã‚‰æ¨æ¸¬ï¼‰
            supported_methods = []
            model_id_lower = model.id.lower()

            # Chat/Completionså¯¾å¿œ
            if any(
                model.id.startswith(p)
                for p in ["gpt-", "o1-", "o3-", "o4-", "chatgpt-"]
            ):
                supported_methods.append("generateContent")

            # Audioå¯¾å¿œï¼ˆtranscribe = speech-to-text, tts = text-to-speechï¼‰
            if "transcribe" in model_id_lower:
                supported_methods.append("transcribe")
            if "tts" in model_id_lower:
                supported_methods.append("textToSpeech")
            if "audio" in model_id_lower or "realtime" in model_id_lower:
                supported_methods.append("audio")

            # Vision/Multimodalå¯¾å¿œ
            if supports_vision:
                supported_methods.append("vision")

            models_list.append(
                {
                    "id": f"openai/{model.id}",
                    "name": model.id,
                    "provider": "OpenAI",
                    "litellm_provider": "openai",
                    "supports_vision": supports_vision,
                    "supports_json": True,
                    "recommended": recommended,
                    "supported_methods": supported_methods,
                    "description": f"OpenAI {model.id}",
                    "cost_per_1k_tokens": {"input": 0.0, "output": 0.0},
                }
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        _MODEL_CACHE[cache_key] = models_list
        _CACHE_EXPIRY[cache_key] = datetime.now() + CACHE_TTL

        logger.info("âœ… Found %d OpenAI chat models from API", len(models_list))
        return models_list

    except ImportError as e:
        logger.warning("openai package not installed: %s", e)
        logger.info("Install with: pip install -U openai")
        return []
    except Exception as e:
        logger.warning("OpenAI model discovery failed: %s: %s", type(e).__name__, e)
        return []


def clear_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
    global _MODEL_CACHE, _CACHE_EXPIRY
    _MODEL_CACHE.clear()
    _CACHE_EXPIRY.clear()
    logger.info("Model cache cleared")
