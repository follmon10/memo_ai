"""
å‹•çš„ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼APIã‹ã‚‰å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹
"""
import os
from typing import List, Dict, Any
from datetime import datetime, timedelta

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
_MODEL_CACHE: Dict[str, List[Dict[str, Any]]] = {}
_CACHE_EXPIRY: Dict[str, datetime] = {}
CACHE_TTL = timedelta(hours=1)


def get_gemini_models() -> List[Dict[str, Any]]:
    """
    Gemini APIã‹ã‚‰å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å‹•çš„ã«å–å¾—
    
    ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å¯¾å¿œ (2024):
    - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆ1æ™‚é–“TTLï¼‰
    - ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
    - ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼ˆèµ·å‹•æ™‚1å›ã®ã¿ï¼‰
    - ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆsupported_generation_methodsï¼‰
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆã€‚å„ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®æ§‹é€ :
        {
            "id": "gemini/gemini-2.5-flash",
            "name": "gemini-2.5-flash",
            "provider": "Gemini API",
            "litellm_provider": "gemini",
            "supports_vision": True,
            "supports_json": True,
            "description": "...",
            "cost_per_1k_tokens": {"input": 0.0, "output": 0.0}
        }
    """
    cache_key = "gemini_models_v3"  # v3: æ•™è‚²ç”¨éæ¨å¥¨ãƒ¢ãƒ‡ãƒ«è¿½åŠ 
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if cache_key in _MODEL_CACHE:
        if datetime.now() < _CACHE_EXPIRY[cache_key]:
            cached_count = len(_MODEL_CACHE[cache_key])
            print(f"[INFO] ğŸ’¾ Using cached Gemini models ({cached_count} models)")
            return _MODEL_CACHE[cache_key]
    
    # Gemini APIã‹ã‚‰å–å¾—ï¼ˆã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
    try:
        # æ–°ã—ã„ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: google-genai (2024+ recommended)
        import google.genai as genai
        import time
        
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("[WARNING] GEMINI_API_KEY not set")
            return []
        
        max_retries = 3
        models = []
        
        for attempt in range(max_retries):
            try:
                # google-genai æ–°SDKï¼ˆClient APIã‚’ä½¿ç”¨ï¼‰
                client = genai.Client(api_key=api_key)
                
                # client.models.list()ã§ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«ï¼‰
                # 
                # ===== Gemini API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (2024_12+) =====
                # æ–°SDKãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ä»¥ä¸‹ã®å±æ€§ãŒåˆ©ç”¨å¯èƒ½:
                # - name: ãƒ¢ãƒ‡ãƒ«ID (ä¾‹: "models/gemini-2.5-flash")
                # - display_name: è¡¨ç¤ºå
                # - description: ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜
                # - supported_actions: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹æ©Ÿèƒ½ã®ãƒªã‚¹ãƒˆ (NEW)
                #   æ—§SDKã§ã¯ supported_generation_methods
                #   ä¾‹: ['generateContent', 'streamGenerateContent', ...]
                # - input_token_limit: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
                # - output_token_limit: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
                # - temperature, top_k, top_p: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                # - thinking: Thinkingæ©Ÿèƒ½ã®ã‚µãƒãƒ¼ãƒˆ (ä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ã®ã¿)
                # - endpoints: åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
                # - labels: ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ãƒ™ãƒ«ãƒ»ã‚¿ã‚°
                # ãã®ä»–: checkpoints, tuned_model_info, default_checkpoint_id, etc.
                # =====================================================
                for model in client.models.list():
                    # æ–°SDKã§ã¯supported_actionsã€æ—§SDKã§ã¯supported_generation_methods
                    methods = getattr(model, 'supported_actions', None)
                    if methods is None:
                        methods = getattr(model, 'supported_generation_methods', None)
                    if methods is None:
                        continue
                        
                    model_name = model.name.split('/')[-1]  # "models/gemini-pro" -> "gemini-pro"
                    
                    # ãƒãƒ£ãƒƒãƒˆç”¨é€”ï¼ˆgenerateContentï¼‰ã‹ã©ã†ã‹ã§æ¨å¥¨åˆ¤å®š
                    is_recommended = 'generateContent' in methods
                    
                    # Visionå¯¾å¿œã®åˆ¤å®š: ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¤å®š
                    # Gemini APIã¯input_token_limitã‚„supported_modesã§åˆ¤å®šå¯èƒ½
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: generateContentãŒã‚ã‚‹å ´åˆã¯åŸºæœ¬çš„ã«Visionå¯¾å¿œã¨ä»®å®š
                    supports_vision = False
                    if hasattr(model, 'supported_modes'):
                        # æ–°ã—ã„APIã§ã¯supported_modesã§åˆ¤å®š
                        supports_vision = any('vision' in str(mode).lower() for mode in model.supported_modes)
                    elif 'generateContent' in methods:
                        # generateContentãŒã‚ã‚Œã°ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆVisionå¯¾å¿œï¼‰ã®å¯èƒ½æ€§ãŒé«˜ã„
                        # ãŸã ã—ã€embeddingç³»ã¯é™¤å¤–
                        supports_vision = 'embed' not in model_name.lower()
                    
                    models.append({
                        "id": f"gemini/{model_name}",
                        "name": model_name,
                        "provider": "Gemini API",
                        "litellm_provider": "gemini",
                        "supports_vision": supports_vision,
                        "supports_json": True,
                        "description": getattr(model, 'description', ''),
                        "recommended": is_recommended,
                        "supported_methods": list(methods),  # ãƒ‡ãƒãƒƒã‚°ç”¨
                        "cost_per_1k_tokens": {
                            "input": 0.0,
                            "output": 0.0
                        }
                    })
                
                # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                break
                
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # 1ç§’, 2ç§’, 4ç§’
                    print(f"[WARNING] âš ï¸ Retry {attempt + 1}/{max_retries} after {wait_time}s: {type(e).__name__}: {e}")
                    time.sleep(wait_time)
                else:
                    # æœ€çµ‚ãƒªãƒˆãƒ©ã‚¤å¤±æ•—
                    print(f"[ERROR] âŒ Failed after {max_retries} attempts: {type(e).__name__}: {e}")
                    raise
        
        if not models:
            print("[WARNING] No Gemini models found from API")
            return []
        
        print(f"[INFO] âœ… Fetched {len(models)} Gemini models from API")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆ1æ™‚é–“TTLï¼‰
        _MODEL_CACHE[cache_key] = models
        _CACHE_EXPIRY[cache_key] = datetime.now() + CACHE_TTL
        
        return models
        
    except ImportError as e:
        print(f"[WARNING] google-genai package not installed: {e}")
        print("[INFO] Install with: pip install -U google-genai")
        return []
    except Exception as e:
        print(f"[ERROR] Failed to fetch Gemini models: {type(e).__name__}: {e}")
        return []


def get_openai_models() -> List[Dict[str, Any]]:
    """
    OpenAI APIã‹ã‚‰å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å‹•çš„ã«å–å¾—
    
    ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹å¯¾å¿œ:
    - APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰
    - APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆå„ªé›…ãªå¤±æ•—ï¼‰
    - ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆgpt-, o1-, chatgpt-ï¼‰
    - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼ˆ1æ™‚é–“TTLï¼‰
    
    Returns:
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ãƒªã‚¹ãƒˆï¼ˆAPIã‚­ãƒ¼ãªã—ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆï¼‰
    """
    cache_key = "openai_models_v1"
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    if cache_key in _MODEL_CACHE:
        if datetime.now() < _CACHE_EXPIRY.get(cache_key, datetime.min):
            cached_count = len(_MODEL_CACHE[cache_key])
            print(f"[INFO] ğŸ’¾ Using cached OpenAI models ({cached_count} models)")
            return _MODEL_CACHE[cache_key]
    
    # APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("[INFO] OPENAI_API_KEY not set, skipping OpenAI models")
        return []
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        
        models_list = []
        
        # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        all_models = client.models.list()
        
        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆç ”ç©¶çµæœã«åŸºã¥ãï¼‰
        chat_prefixes = ['gpt-', 'o1-', 'o3-', 'o4-', 'chatgpt-']
        
        for model in all_models:
            # ãƒãƒ£ãƒƒãƒˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
            if not any(model.id.startswith(prefix) for prefix in chat_prefixes):
                continue
            
            # Visionå¯¾å¿œåˆ¤å®š
            supports_vision = any(keyword in model.id for keyword in [
                'vision', 'gpt-4o', 'gpt-4-turbo', 'gpt-4.5'
            ])
            
            # æ¨å¥¨ãƒ¢ãƒ‡ãƒ«åˆ¤å®šï¼ˆæœ€æ–°ã®å®‰å®šç‰ˆï¼‰
            # æ˜ç¤ºçš„ã«éæ¨å¥¨ã®ã‚‚ã®ã ã‘ã‚’é™¤å¤–ã™ã‚‹æ–¹å¼ã«å¤‰æ›´
            # Fine-tunedãƒ¢ãƒ‡ãƒ«ã€å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€å®Ÿé¨“çš„ãƒ¢ãƒ‡ãƒ«ãªã©ã‚’é™¤å¤–
            not_recommended_patterns = [
                'ft:',              # Fine-tunedãƒ¢ãƒ‡ãƒ«
                'gpt-4-0613',       # å¤ã„GPT-4ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                'gpt-4-0314',
                'gpt-3.5-turbo-0301',
                'gpt-3.5-turbo-0613',
                'gpt-3.5-turbo-16k-0613',
                '-preview',         # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‰ˆï¼ˆo1-previewãªã©ä¾‹å¤–ã‚ã‚Šï¼‰
                'gpt-5',            # æœªãƒªãƒªãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
                'gpt-image',        # å®Ÿé¨“çš„
                'chatgpt-image',    # å®Ÿé¨“çš„
            ]
            
            # ä¾‹å¤–çš„ã«æ¨å¥¨ã™ã‚‹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ‡ãƒ«
            recommended_previews = ['o1-preview', 'o1-mini']
            
            # åˆ¤å®šï¼šéæ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã—ãªã„ã‹ã€ã¾ãŸã¯ä¾‹å¤–ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹
            is_preview_exception = any(exc in model.id for exc in recommended_previews)
            has_not_recommended_pattern = any(pattern in model.id for pattern in not_recommended_patterns)
            
            recommended = (not has_not_recommended_pattern) or is_preview_exception
            
            # supported_methodsæ¨æ¸¬ï¼ˆOpenAI APIã¯æ©Ÿèƒ½ãƒªã‚¹ãƒˆã‚’è¿”ã•ãªã„ãŸã‚åå‰ã‹ã‚‰æ¨æ¸¬ï¼‰
            supported_methods = []
            model_id_lower = model.id.lower()
            
            # Chat/Completionså¯¾å¿œ
            if any(model.id.startswith(p) for p in ['gpt-', 'o1-', 'o3-', 'o4-', 'chatgpt-']):
                supported_methods.append('generateContent')
            
            # Audioå¯¾å¿œï¼ˆtranscribe = speech-to-text, tts = text-to-speechï¼‰
            if 'transcribe' in model_id_lower:
                supported_methods.append('transcribe')
            if 'tts' in model_id_lower:
                supported_methods.append('textToSpeech')
            if 'audio' in model_id_lower or 'realtime' in model_id_lower:
                supported_methods.append('audio')
            
            # Vision/Multimodalå¯¾å¿œ
            if supports_vision:
                supported_methods.append('vision')
            
            models_list.append({
                "id": f"openai/{model.id}",
                "name": model.id,
                "provider": "OpenAI",
                "litellm_provider": "openai",
                "supports_vision": supports_vision,
                "supports_json": True,
                "recommended": recommended,
                "supported_methods": supported_methods,
                "description": f"OpenAI {model.id}",
                "cost_per_1k_tokens": {"input": 0.0, "output": 0.0}
            })
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        _MODEL_CACHE[cache_key] = models_list
        _CACHE_EXPIRY[cache_key] = datetime.now() + CACHE_TTL
        
        print(f"[INFO] âœ… Found {len(models_list)} OpenAI chat models from API")
        return models_list
        
    except ImportError as e:
        print(f"[WARNING] openai package not installed: {e}")
        print("[INFO] Install with: pip install -U openai")
        return []
    except Exception as e:
        print(f"[WARNING] OpenAI model discovery failed: {type(e).__name__}: {e}")
        return []


def clear_cache():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
    global _MODEL_CACHE, _CACHE_EXPIRY
    _MODEL_CACHE.clear()
    _CACHE_EXPIRY.clear()
    print("[INFO] Model cache cleared")
